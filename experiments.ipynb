{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instance():\n",
    "    T = 1000\n",
    "    N = 5\n",
    "    d = 2\n",
    "    n_explore = N\n",
    "    reward_noise_level = 0.1\n",
    "    covariate_diversity=True\n",
    "    \n",
    "    mu_true = np.random.randn(N,d)\n",
    "    contexts = get_contexts(d,T,covariate_diversity)\n",
    "\n",
    "    return {'T':T,'N':N,'d':d,'n_explore':n_explore,\n",
    "          'reward_noise_level':reward_noise_level,\n",
    "          'mu_true':mu_true,'contexts':contexts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_noise(reward_noise_level=0.1):\n",
    "    '''noise in the reward signal'''\n",
    "    return reward_noise_level*np.random.randn()\n",
    "\n",
    "def get_choice_myopic(context,mu_x,payments=None):\n",
    "    '''agents are myopic but deterministic. tbd: stochastic'''\n",
    "    temp = np.dot(mu_x,context)\n",
    "    if payments is not None:\n",
    "        temp += payments\n",
    "    return temp.argmax()\n",
    "\n",
    "def get_contexts(d,T,covariate_diversity=True):\n",
    "    '''pre-generated contexts'''\n",
    "    contexts = {}\n",
    "    if covariate_diversity is True:\n",
    "        for t in range(T):\n",
    "            contexts[t] = np.random.randn(d)\n",
    "        return contexts\n",
    "    else:\n",
    "        mean = np.ones(d)\n",
    "        cov0 = 0.1*np.random.randn(d,d)\n",
    "        cov = np.ones((d,d)) + np.dot(cov0,cov0.transpose())\n",
    "\n",
    "        for t in range(T):\n",
    "            contexts[t] = np.random.multivariate_normal(mean, cov)\n",
    "        return contexts    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(instance,platform):\n",
    "\n",
    "    mu_true = instance['mu_true']\n",
    "    N = instance['N']\n",
    "    d = instance['d']\n",
    "    T = instance['T']\n",
    "    n_explore = instance['n_explore']\n",
    "    contexts = instance['contexts']\n",
    "    \n",
    "    #variables for logging\n",
    "    pseudoregret_inst = np.zeros(T)\n",
    "    history = {i:{'X':[],'y':[],'t':[],'payments':[]} for i in range(N)} #data for each arm\n",
    "    history['meta'] = {'N':N,'n_explore':n_explore,'prev_choice':None,'prev_mu_estimated':np.zeros_like(mu_true)}\n",
    "    \n",
    "    #T interactions\n",
    "    for t in range(T):\n",
    "        #agent arrives\n",
    "        context = contexts[t]\n",
    "\n",
    "        #platform estimates mus and decides payments based on history\n",
    "        payments,mu_estimated,initial_forced_choice = platform(t,context,history)\n",
    "        \n",
    "        if initial_forced_choice is not None: #do initial forced exploration\n",
    "            choice = initial_forced_choice\n",
    "        else: #agent decides arm\n",
    "            choice = get_choice_myopic(context,mu_estimated,payments)\n",
    "        \n",
    "        #reward information is revealed\n",
    "        reward_realized = np.dot(mu_true[choice],context) + reward_noise()\n",
    "\n",
    "        #data collection\n",
    "        history[choice]['X'].append(context)\n",
    "        history[choice]['y'].append(reward_realized)\n",
    "        history[choice]['t'].append(t)\n",
    "        history[choice]['payments'].append(payments)\n",
    "        history['meta']['prev_choice'] = choice\n",
    "        history['meta']['prev_mu_estimated'] = mu_estimated\n",
    "        \n",
    "        #regret computation\n",
    "        opt_choice = get_choice_myopic(context,mu_true,None)\n",
    "        pseudoregret_inst[t] = np.dot(mu_true[opt_choice] - mu_true[choice],context)\n",
    "\n",
    "    assert pseudoregret_inst.min() >= 0\n",
    "    pseudoregret = np.cumsum(pseudoregret_inst)\n",
    "    return pseudoregret,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mu(t,history):\n",
    "    mu_estimated = history['meta']['prev_mu_estimated']\n",
    "    update_index = history['meta']['prev_choice']\n",
    "    X = np.array(history[update_index]['X'])\n",
    "    y = np.array(history[update_index]['y'])\n",
    "    \n",
    "    mu_estimated[update_index] = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    \n",
    "    return mu_estimated\n",
    "    \n",
    "def platform_greedy(t,context,history):\n",
    "    n_explore = history['meta']['n_explore']\n",
    "    N = history['meta']['N']\n",
    "    \n",
    "    if t< n_explore*N: #assign context to each arm in a round robin fashion:\n",
    "        initial_forced_choice = t%N\n",
    "        return None,history['meta']['prev_mu_estimated'], initial_forced_choice\n",
    "    else:\n",
    "        mu_estimated = estimate_mu(t,history)\n",
    "        return None,mu_estimated, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_runs = 5\n",
    "algos = {'greedy':platform_greedy}\n",
    "prs_all = {}\n",
    "history_all = {}\n",
    "for algo in algos:\n",
    "    prs_all[algo] = [0]*n_mc_runs\n",
    "    history_all[algo] = {}\n",
    "instance = {}\n",
    "for mc_run in range(n_mc_runs):\n",
    "    instance[mc_run] = generate_instance()\n",
    "    for algo in algos:\n",
    "        prs_all[algo][mc_run],history_all[algo][mc_run] = interactions(instance = instance[mc_run],platform = algos[algo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e,algo in enumerate(algos):\n",
    "    plt.figure()\n",
    "    print(algo)\n",
    "    plt.plot(np.mean(prs_all[algo],axis=0),label=algo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[history_all['greedy'][x]['meta']['prev_mu_estimated'] for x in history_all['greedy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[instance[x]['mu_true'] for x in instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e,algo in enumerate(algos):\n",
    "    plt.figure()\n",
    "    print(algo)\n",
    "    plt.plot(np.array(prs_all[algo]).transpose(),label=algo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
